\section{Access to Hardware Resources}
\label{sec:hardware-resources}

\comment{db: I suspect we'll need a statement like this.  Not sure
  exactly where it will go yet.}

The proposed work will require access to hardware resources for
development, testing, and evaluation of the OS/R environment.  This
includes both novel or prototype hardware, representing key
characteristics of forthcoming systems which must be supported by the
exascale OS/R, as well as the largest scales of HPC systems available
to allow at-scale testing and evaluation.  We anticipate taking
several approaches to obtain the necessary access to hardware
resources.

For at-scale testing on current hardware, we expect to take advantage
of the ASCR production facilities, specifically the Argonne and Oak
Ridge Leadership Computing Facilities (ALCF \cite{alcf:www}, OLCF
\cite{olcf:www}) and possibly also the National Energy Research
Scientific Computing Center (NERSC) \cite{nersc:www} at LBNL.  Time on
these systems is allocated through several programs
\cite{incite:www,alcc:www,ercap:www}, including the locally-controlled
10\% Director's Discretion, all of which are proposal-based.  While
system software research is not common on these systems, team members
have been successful in the past in obtaining allocations through
these programs \cite{Laros2012,Colony:www,XVirt:2012:DD,Colony:INCITE,Colony:ALCC,Colony:DD} and in working
with center staff to coordinate ``test shots'', typically as systems
are going into or out of maintenance, to obtain useful research
results
\cite{kelly:2008:catamount-n-way,ferreira:noise-injection,kelly:2012:path-to-exascale}.

%% All of the largest HPC systems available within the DOE complex are
%% considered production resources.  This includes the Leadership
%% Computing Facility (LCF) systems at Argonne (ALCF) \cite{alcf:www} and
%% Oak Ridge (OLCF) \cite{olcf:www}, as well as the National Energy
%% Research Scientific Computing Center (NERSC) \cite{nersc:www} at LBNL.
%% While systems software research is challenging on production systems,
%% but there is precedent for it at all three centers, typically as the
%% machine goes into and out of maintenance periods.  \comment{Need to
%%   verify: OLCF yes! ALCF? NERSC?}  Time on these systems is allocated
%% through several programs: INCITE \cite{incite:www} is a science-based
%% peer-reviewed program used at the LCF centers, while ALCC
%% \cite{alcc:www} (at all three centers) and ERCAP \cite{ercap:www} (at
%% NERSC) are run by the DOE Advanced Scientific Computing Research
%% (ASCR) office targeting DOE mission needs.  Additionally each of the
%% centers retains 10\% of the systems for local discretionary
%% allocations. We plan to make use of all three of these programs to
%% obtain the necessary access for at-scale testing.

%% Team members have successful backgrounds in obtaining allocations through
%% both the ALCC and INCITE programs \cite{Colony:www}. This experience
%% has provided helpful knowledge and skills for the performance of
%% kernel-based and runtime system-based demonstrations on leadership-class
%% production machines.  \comment{Please add additional allocation awards
%%   with ALCC/INCITE for systems research.}

%% As part of a 2010 incite award titled, ``System Software Research for
%% Extreme-Scale Computing'', a Sandia team ran a number of dedicated-access
%% experiments on the OLCF Jaguar system.  These experiments included a
%% demonstration of the capabilities of the Catamount N-Way
%% kernel~\cite{kelly:2008:catamount-n-way}, OS noise
%% studies~\cite{ferreira:noise-injection}, power and energy
%% studies~\cite{Laros2012}, a demonstration of ``fast'' debugging
%% capabilities~\cite{kelly:2012:path-to-exascale}, and large-scale virtual machine
%% experiments.

%% % TJN: ORNL's DD for X-Stack virt testing on RIZZO/CHESTER (XVirt:2012:DD)
%% Moreover, additional experience is derived from our work through the
%% Director's Discretionary awards ~\cite{XVirt:2012:DD, Colony:www}.
%% \comment{Add citation for your successful systems
%%   research DD allocations.}.

In addition to at-scale testing, there is a need for access to
examples of the latest hardware in order to develop and test support
for new features or characteristics of the hardware.  Generally, this
is at the node level, so a small system with a few nodes will
typically suffice; simulators can also be useful for this kind of
work.  Many of the participating institutions have, and regularly
acquire, new leading-edge systems (see Sec.~\vref{sec:facilities} for
current systems), which we expect to make use of.  One such system of
note is LANL's Darwin advanced architecture testbed, which hosts
modern GPU and manycore processors, and is updated frequently as
hardware prototypes become available.  SNL also hosts a variety of
experimental systems, including Intel MIC, AMD Llano Fusion, and
Tilera hardware.  ORNL also has available a variety of multicore and
accelerator-based systems, as well as experimental nonvolatile memory
and interconnect systems.  We also plan to work with hardware vendors
to access earlier-stage prototypes and simulators.

%% We also have beta-test access to NSF PRObE systems (approx. 1000 nodes) which
%% also allows custom kernel installation and privileged access which will be useful for initial development.

%% The Experimental Computing Laboratory (ExCL) \url{http://j.mp/Nk7NU7} is managed by the Future Technologies Group at ORNL, and provides its user community with testbeds for evaluating emerging architectures. ExCL researchers, including both internal and external scientists, use these testbeds to investigate architectures, such as multicore processors from Intel, AMD, and ARM, Graphics Processing Units (GPUs), Field Programmable Gate Arrays (FPGAs), nonvolatile memory, and interconnection networks. ExCL also hosts a variety of simulation and tool platforms for detailed investigations of emerging architectures. Most hardware is located in an access-controlled server room at ORNL in building 5100, near the Future Technology Group.

